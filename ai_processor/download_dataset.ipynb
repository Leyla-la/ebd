{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3809f723",
   "metadata": {},
   "source": [
    "# Edinburgh Office Dataset Downloader & Google Drive Mounter\n",
    "\n",
    "**Dataset:** https://homepages.inf.ed.ac.uk/rbf/OFFICEDATA/\n",
    "\n",
    "This script downloads the Edinburgh office monitoring video dataset and mounts it to Google Drive for easy access.\n",
    "\n",
    "**License:** CC BY-NC-SA (Attribution-NonCommercial-ShareAlike)\n",
    "\n",
    "**Citation required:** T. Qasim, R. B. Fisher, N. Bhatti; Ground-truthing Large Human Behavior Monitoring Datasets, Proc. 2020 Int. Conf on Pattern Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a095b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import tarfile\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm.notebook import tqdm  # Use notebook-friendly tqdm\n",
    "import hashlib\n",
    "\n",
    "# Google Colab specific imports\n",
    "try:\n",
    "    from google.colab import drive, files\n",
    "    import shutil\n",
    "    COLAB_ENV = True\n",
    "    print(\"‚úÖ Detected Google Colab environment\")\n",
    "except ImportError:\n",
    "    COLAB_ENV = False\n",
    "    print(\"‚ùå Not in Google Colab - will download locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de956760",
   "metadata": {},
   "source": [
    "## Downloader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8747735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdinburghOfficeDownloader:\n",
    "    def __init__(self, base_url=\"https://homepages.inf.ed.ac.uk/rbf/OFFICEDATA/\"):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "\n",
    "        # Setup directories\n",
    "        if COLAB_ENV:\n",
    "            self.local_dir = Path('/content/edinburgh_office_dataset')\n",
    "            self.gdrive_dir = Path('/content/drive/MyDrive/Datasets/EdinburghOffice')\n",
    "        else:\n",
    "            self.local_dir = Path('./edinburgh_office_dataset')\n",
    "            self.gdrive_dir = None\n",
    "\n",
    "        self.local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def mount_google_drive(self):\n",
    "        \"\"\"Mount Google Drive if in Colab\"\"\"\n",
    "        if not COLAB_ENV:\n",
    "            print(\"‚ö†Ô∏è Not in Colab environment - skipping Google Drive mount\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            print(\"üìÅ Mounting Google Drive...\")\n",
    "            drive.mount('/content/drive')\n",
    "\n",
    "            # Create dataset directory in Google Drive\n",
    "            self.gdrive_dir.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"‚úÖ Google Drive mounted. Dataset will be saved to: {self.gdrive_dir}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to mount Google Drive: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_file_list(self):\n",
    "        \"\"\"Scrape the dataset webpage to get list of available files\"\"\"\n",
    "        print(f\"üîç Scanning dataset webpage: {self.base_url}\")\n",
    "\n",
    "        try:\n",
    "            response = self.session.get(self.base_url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            files = []\n",
    "\n",
    "            # Find all links to downloadable files\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                # Look for common dataset file extensions\n",
    "                if any(href.lower().endswith(ext) for ext in [\n",
    "                    '.zip', '.tar.gz', '.tar', '.rar', '.7z', '.mp4', '.avi',\n",
    "                    '.mov', '.jpg', '.jpeg', '.png', '.mat', '.txt', '.csv'\n",
    "                ]):\n",
    "                    full_url = urllib.parse.urljoin(self.base_url, href)\n",
    "                    files.append({\n",
    "                        'url': full_url,\n",
    "                        'filename': os.path.basename(href),\n",
    "                        'link_text': link.get_text().strip()\n",
    "                    })\n",
    "\n",
    "            # Also look for subdirectories\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.endswith('/') and not href.startswith('http') and href != '../':\n",
    "                    subdir_url = urllib.parse.urljoin(self.base_url, href)\n",
    "                    print(f\"üîç Found subdirectory: {subdir_url}\")\n",
    "                    # Recursively scan subdirectories\n",
    "                    subfiles = self.scan_directory(subdir_url)\n",
    "                    files.extend(subfiles)\n",
    "\n",
    "            print(f\"‚úÖ Found {len(files)} files to download\")\n",
    "            return files\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error scanning webpage: {e}\")\n",
    "            return []\n",
    "\n",
    "    def scan_directory(self, dir_url):\n",
    "        \"\"\"Recursively scan a directory for files\"\"\"\n",
    "        files = []\n",
    "        try:\n",
    "            response = self.session.get(dir_url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if any(href.lower().endswith(ext) for ext in [\n",
    "                    '.zip', '.tar.gz', '.tar', '.rar', '.7z', '.mp4', '.avi',\n",
    "                    '.mov', '.jpg', '.jpeg', '.png', '.mat', '.txt', '.csv'\n",
    "                ]):\n",
    "                    full_url = urllib.parse.urljoin(dir_url, href)\n",
    "                    files.append({\n",
    "                        'url': full_url,\n",
    "                        'filename': os.path.basename(href),\n",
    "                        'link_text': link.get_text().strip(),\n",
    "                        'subdir': dir_url.replace(self.base_url, '')\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error scanning directory {dir_url}: {e}\")\n",
    "\n",
    "        return files\n",
    "\n",
    "    def download_file(self, file_info, destination_dir):\n",
    "        \"\"\"Download a single file with progress bar\"\"\"\n",
    "        url = file_info['url']\n",
    "        filename = file_info['filename']\n",
    "\n",
    "        # Handle subdirectories\n",
    "        if 'subdir' in file_info and file_info['subdir']:\n",
    "            subdir_path = destination_dir / file_info['subdir']\n",
    "            subdir_path.mkdir(parents=True, exist_ok=True)\n",
    "            filepath = subdir_path / filename\n",
    "        else:\n",
    "            filepath = destination_dir / filename\n",
    "\n",
    "        # Skip if file already exists and has the same size\n",
    "        if filepath.exists():\n",
    "            try:\n",
    "                head_response = self.session.head(url, timeout=30)\n",
    "                remote_size = int(head_response.headers.get('content-length', 0))\n",
    "                local_size = filepath.stat().st_size\n",
    "\n",
    "                if remote_size > 0 and local_size == remote_size:\n",
    "                    print(f\"‚è≠Ô∏è Skipping {filename} (already downloaded)\")\n",
    "                    return True\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        print(f\"üì• Downloading: {filename}\")\n",
    "\n",
    "        try:\n",
    "            response = self.session.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "            with open(filepath, 'wb') as f:\n",
    "                if total_size > 0:\n",
    "                    with tqdm(total=total_size, unit='B', unit_scale=True, desc=filename) as pbar:\n",
    "                        for chunk in response.iter_content(chunk_size=8192):\n",
    "                            if chunk:\n",
    "                                f.write(chunk)\n",
    "                                pbar.update(len(chunk))\n",
    "                else:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "\n",
    "            print(f\"‚úÖ Downloaded: {filename}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to download {filename}: {e}\")\n",
    "            if filepath.exists():\n",
    "                filepath.unlink()  # Remove partial download\n",
    "            return False\n",
    "\n",
    "    def extract_archives(self, directory):\n",
    "        \"\"\"Extract any downloaded archive files\"\"\"\n",
    "        print(\"üì¶ Extracting archive files...\")\n",
    "\n",
    "        archive_files = []\n",
    "        for ext in ['.zip', '.tar.gz', '.tar', '.rar', '.7z']:\n",
    "            archive_files.extend(directory.glob(f\"**/*{ext}\"))\n",
    "\n",
    "        for archive_path in archive_files:\n",
    "            print(f\"üì¶ Extracting: {archive_path.name}\")\n",
    "\n",
    "            extract_dir = archive_path.parent / archive_path.stem\n",
    "            extract_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            try:\n",
    "                if archive_path.suffix == '.zip':\n",
    "                    with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(extract_dir)\n",
    "                elif archive_path.name.endswith('.tar.gz') or archive_path.name.endswith('.tar'):\n",
    "                    with tarfile.open(archive_path, 'r:*') as tar_ref:\n",
    "                        tar_ref.extractall(extract_dir)\n",
    "\n",
    "                print(f\"‚úÖ Extracted: {archive_path.name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to extract {archive_path.name}: {e}\")\n",
    "\n",
    "    def sync_to_gdrive(self):\n",
    "        \"\"\"Copy downloaded files to Google Drive\"\"\"\n",
    "        if not COLAB_ENV or not self.gdrive_dir:\n",
    "            print(\"‚ö†Ô∏è Google Drive not available - files saved locally only\")\n",
    "            return\n",
    "\n",
    "        print(\"‚òÅÔ∏è Syncing to Google Drive...\")\n",
    "\n",
    "        try:\n",
    "            # Copy entire dataset directory to Google Drive\n",
    "            if self.gdrive_dir.exists():\n",
    "                print(f\"Path {self.gdrive_dir} exists. Removing before copying.\")\n",
    "                shutil.rmtree(self.gdrive_dir)\n",
    "\n",
    "            shutil.copytree(self.local_dir, self.gdrive_dir)\n",
    "            print(f\"‚úÖ Dataset synced to Google Drive: {self.gdrive_dir}\")\n",
    "\n",
    "            # Create a README with dataset info\n",
    "            readme_content = \"\"\"\n",
    "# Edinburgh Office Monitoring Dataset\n",
    "\n",
    "This dataset contains low frame rate video of people doing their normal activities\n",
    "in an office setting. The data is acquired using a fixed camera as a set of\n",
    "1280*720 pixel color images captured at an average of about 1 FPS.\n",
    "\n",
    "## License\n",
    "Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)\n",
    "\n",
    "## Citation\n",
    "T. Qasim, R. B. Fisher, N. Bhatti; Ground-truthing Large Human Behavior\n",
    "Monitoring Datasets, Proc. 2020 Int. Conf on Pattern Recognition, online, 2021.\n",
    "\n",
    "## Acknowledgment\n",
    "\"We thank the University of Edinburgh for the use of the low resolution\n",
    "video and ground truth data.\"\n",
    "\n",
    "## Contact\n",
    "Robert Fisher at rbf@inf.ed.ac.uk\n",
    "School of Informatics, University of Edinburgh\n",
    "\n",
    "## Downloaded\n",
    "Dataset downloaded and mounted on: \"\"\" + time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            readme_path = self.gdrive_dir / 'README.md'\n",
    "            readme_path.write_text(readme_content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to sync to Google Drive: {e}\")\n",
    "\n",
    "    def generate_file_manifest(self):\n",
    "        \"\"\"Generate a manifest of downloaded files\"\"\"\n",
    "        print(\"üìã Generating file manifest...\")\n",
    "\n",
    "        manifest = []\n",
    "        for filepath in self.local_dir.rglob('*'):\n",
    "            if filepath.is_file():\n",
    "                # Calculate file hash for integrity checking\n",
    "                sha256_hash = hashlib.sha256()\n",
    "                with open(filepath, \"rb\") as f:\n",
    "                    for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "                        sha256_hash.update(byte_block)\n",
    "\n",
    "                manifest.append({\n",
    "                    'file': str(filepath.relative_to(self.local_dir)),\n",
    "                    'size': filepath.stat().st_size,\n",
    "                    'sha256': sha256_hash.hexdigest()\n",
    "                })\n",
    "\n",
    "        # Save manifest\n",
    "        manifest_path = self.local_dir / 'file_manifest.txt'\n",
    "        with open(manifest_path, 'w') as f:\n",
    "            f.write(\"Edinburgh Office Dataset - File Manifest\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "            total_size = 0\n",
    "            for item in manifest:\n",
    "                f.write(f\"File: {item['file']}\\n\")\n",
    "                f.write(f\"Size: {item['size']:,} bytes\\n\")\n",
    "                f.write(f\"SHA256: {item['sha256']}\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                total_size += item['size']\n",
    "\n",
    "            f.write(f\"\\nTotal files: {len(manifest)}\\n\")\n",
    "            f.write(f\"Total size: {total_size:,} bytes ({total_size/1024/1024:.1f} MB)\\n\")\n",
    "\n",
    "        print(f\"‚úÖ File manifest saved: {manifest_path}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main execution function\"\"\"\n",
    "        print(\"üöÄ Starting Edinburgh Office Dataset Download\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Mount Google Drive if in Colab\n",
    "        if COLAB_ENV:\n",
    "            self.mount_google_drive()\n",
    "\n",
    "        # Get list of files to download\n",
    "        files_to_download = self.get_file_list()\n",
    "\n",
    "        if not files_to_download:\n",
    "            print(\"‚ùå No files found to download. Please check the dataset URL.\")\n",
    "            return False\n",
    "\n",
    "        # Download files\n",
    "        print(f\"\\nüì• Starting download of {len(files_to_download)} files...\")\n",
    "        successful_downloads = 0\n",
    "\n",
    "        for i, file_info in enumerate(files_to_download, 1):\n",
    "            print(f\"\\n[{i}/{len(files_to_download)}] \", end=\"\")\n",
    "            if self.download_file(file_info, self.local_dir):\n",
    "                successful_downloads += 1\n",
    "\n",
    "            # Add a small delay to be respectful to the server\n",
    "            time.sleep(1)\n",
    "\n",
    "        print(f\"\\n‚úÖ Successfully downloaded {successful_downloads}/{len(files_to_download)} files\")\n",
    "\n",
    "        # Extract archives\n",
    "        self.extract_archives(self.local_dir)\n",
    "\n",
    "        # Generate file manifest\n",
    "        self.generate_file_manifest()\n",
    "\n",
    "        # Sync to Google Drive\n",
    "        if COLAB_ENV:\n",
    "            self.sync_to_gdrive()\n",
    "\n",
    "        print(\"\\nüéâ Dataset download and setup complete!\")\n",
    "        print(f\"üìÅ Local directory: {self.local_dir}\")\n",
    "        if self.gdrive_dir:\n",
    "            print(f\"‚òÅÔ∏è Google Drive directory: {self.gdrive_dir}\")\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944ed45",
   "metadata": {},
   "source": [
    "## Execute Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a59ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize downloader\n",
    "downloader = EdinburghOfficeDownloader()\n",
    "\n",
    "# Run the download process\n",
    "success = downloader.run()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n‚úÖ All done! Your dataset is ready to use.\")\n",
    "    if COLAB_ENV:\n",
    "        print(\"\\nüìã Quick Start Guide:\")\n",
    "        print(\"1. Navigate to your Google Drive\")\n",
    "        print(\"2. Open the 'Datasets/EdinburghOffice' folder\")\n",
    "        print(\"3. Check the README.md for dataset information\")\n",
    "        print(\"4. Use the file_manifest.txt to verify file integrity\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Download failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac928f34",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b231e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_info():\n",
    "    \"\"\"Load basic information about the downloaded dataset\"\"\"\n",
    "    if COLAB_ENV:\n",
    "        dataset_path = Path('/content/drive/MyDrive/Datasets/EdinburghOffice')\n",
    "    else:\n",
    "        dataset_path = Path('./edinburgh_office_dataset')\n",
    "\n",
    "    if not dataset_path.exists():\n",
    "        print(\"‚ùå Dataset not found. Please run the downloader first.\")\n",
    "        return None\n",
    "\n",
    "    # Read manifest\n",
    "    manifest_path = dataset_path / 'file_manifest.txt'\n",
    "    if manifest_path.exists():\n",
    "        print(f\"üìã Dataset manifest: {manifest_path}\")\n",
    "        with open(manifest_path, 'r') as f:\n",
    "            print(f.read())\n",
    "\n",
    "    return dataset_path\n",
    "\n",
    "def list_video_files():\n",
    "    \"\"\"List all video files in the dataset\"\"\"\n",
    "    dataset_path = load_dataset_info()\n",
    "    if not dataset_path:\n",
    "        return []\n",
    "\n",
    "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv']\n",
    "    video_files = []\n",
    "\n",
    "    for ext in video_extensions:\n",
    "        video_files.extend(list(dataset_path.rglob(f\"*{ext}\")))\n",
    "\n",
    "    print(f\"üé• Found {len(video_files)} video files:\")\n",
    "    for video in video_files:\n",
    "        print(f\"  - {video.relative_to(dataset_path)}\")\n",
    "\n",
    "    return video_files\n",
    "\n",
    "def list_image_files():\n",
    "    \"\"\"List all image files in the dataset\"\"\"\n",
    "    dataset_path = load_dataset_info()\n",
    "    if not dataset_path:\n",
    "        return []\n",
    "\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "    image_files = []\n",
    "\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(list(dataset_path.rglob(f\"*{ext}\")))\n",
    "\n",
    "    print(f\"üñºÔ∏è Found {len(image_files)} image files:\")\n",
    "    for image in image_files:\n",
    "        print(f\"  - {image.relative_to(dataset_path)}\")\n",
    "\n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e096489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the utility functions\n",
    "list_video_files();\n",
    "list_image_files();"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
