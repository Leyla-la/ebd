{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3ef9f8",
   "metadata": {},
   "source": [
    "# EBD - AI Model Training & Development\n",
    "\n",
    "This notebook is the central hub for training, evaluating, and exporting the AI models used in the Employee Behavior Detection (EBD) system.\n",
    "\n",
    "**Models to be handled:**\n",
    "1.  **Object Detection (YOLOv8):** For detecting people and desks.\n",
    "2.  **Face Recognition (MTCNN & FaceNet/ArcFace):** For identifying employees.\n",
    "3.  **Person Tracking (DeepSORT):** For tracking individuals across frames.\n",
    "4.  **Activity Classification (CNN/LSTM or Transformer-based):** For classifying employee behaviors.\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Setup:** Mount Google Drive, install dependencies.\n",
    "2.  **Data Loading:** Load the Edinburgh Office Dataset from Google Drive.\n",
    "3.  **Model Training/Fine-tuning:** Train or fine-tune each model.\n",
    "4.  **Evaluation:** Evaluate model performance.\n",
    "5.  **Export:** Save the trained models to Google Drive for the backend to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7533037f",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc2a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    import sys\n",
    "    COLAB_ENV = True\n",
    "    print(\"✅ Detected Google Colab environment. Mounting Google Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "    # Add project path to sys.path for module imports\n",
    "    # sys.path.append('/content/drive/MyDrive/EBD/ai_processor') \n",
    "except ImportError:\n",
    "    COLAB_ENV = False\n",
    "    print(\"❌ Not in Google Colab. Ensure your local environment is set up correctly.\")\n",
    "\n",
    "# Install necessary packages\n",
    "!pip install ultralytics -q\n",
    "!pip install facenet-pytorch -q\n",
    "!pip install deep-sort-realtime -q\n",
    "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html\n",
    "!pip install mmdet\n",
    "!pip install mmpose\n",
    "\n",
    "print(\"✅ Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d89064",
   "metadata": {},
   "source": [
    "## 2. Configuration and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f058a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Define paths\n",
    "if COLAB_ENV:\n",
    "    GDRIVE_PATH = Path('/content/drive/MyDrive/')\n",
    "    DATASET_PATH = GDRIVE_PATH / 'Datasets/EdinburghOffice'\n",
    "    MODEL_SAVE_PATH = GDRIVE_PATH / 'EBD/models'\n",
    "else:\n",
    "    # Adjust for local development if needed\n",
    "    GDRIVE_PATH = Path('./')\n",
    "    DATASET_PATH = GDRIVE_PATH / 'edinburgh_office_dataset'\n",
    "    MODEL_SAVE_PATH = GDRIVE_PATH / 'models'\n",
    "\n",
    "MODEL_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"Model save path: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Verify dataset existence\n",
    "if not DATASET_PATH.exists():\n",
    "    print(\"❌ Dataset not found! Please run the `download_dataset.ipynb` notebook first.\")\n",
    "else:\n",
    "    print(\"✅ Dataset found.\")\n",
    "\n",
    "# Check for GPU\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86fd276",
   "metadata": {},
   "source": [
    "## 3. Person and Desk Detection (YOLOv8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLOv8 model\n",
    "detection_model = YOLO('yolov8n.pt')  # yolov8n is small and fast\n",
    "\n",
    "# It's recommended to fine-tune the model on a custom dataset of office environments\n",
    "# For now, we will use the pre-trained model for demonstration\n",
    "\n",
    "# Example of running inference on an image from the dataset\n",
    "# Note: You'll need to find an actual image file in your dataset\n",
    "try:\n",
    "    image_files = list(DATASET_PATH.rglob('*.jpg')) + list(DATASET_PATH.rglob('*.png'))\n",
    "    if image_files:\n",
    "        sample_image_path = image_files[0]\n",
    "        print(f\"Running detection on sample image: {sample_image_path}\")\n",
    "        results = detection_model(sample_image_path)\n",
    "        results[0].show()\n",
    "    else:\n",
    "        print(\"No sample images found to test detection.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during detection test: {e}\")\n",
    "\n",
    "# Export the model for later use in the backend\n",
    "exported_model_path = detection_model.export(format='onnx') # Export to ONNX for backend flexibility\n",
    "print(f\"Detection model exported to: {exported_model_path}\")\n",
    "\n",
    "# Move to GDrive\n",
    "if COLAB_ENV:\n",
    "    final_path = MODEL_SAVE_PATH / 'yolov8n_detection.onnx'\n",
    "    os.rename(exported_model_path, final_path)\n",
    "    print(f\"Moved to {final_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b75d153",
   "metadata": {},
   "source": [
    "## 4. Face Recognition (MTCNN + FaceNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901029c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize models\n",
    "mtcnn = MTCNN(keep_all=True, device=DEVICE)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(DEVICE)\n",
    "\n",
    "def generate_face_embeddings(image_path):\n",
    "    \"\"\"Detects faces and generates embeddings.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        # Detect faces\n",
    "        boxes, _ = mtcnn.detect(img)\n",
    "        if boxes is None:\n",
    "            print(f\"No faces detected in {image_path.name}\")\n",
    "            return None\n",
    "        \n",
    "        # Get embeddings\n",
    "        faces = mtcnn(img)\n",
    "        if faces is None:\n",
    "            return None\n",
    "        embeddings = resnet(faces.to(DEVICE))\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# This part requires a curated set of employee images for creating a reference database.\n",
    "# For now, we'll just demonstrate the process and save the models.\n",
    "\n",
    "# Save the models for the backend\n",
    "# The models from facenet-pytorch are loaded from torch.hub, so for the backend,\n",
    "# we just need to ensure the library is installed.\n",
    "# We can, however, save the state_dict if we fine-tune them.\n",
    "\n",
    "torch.save(resnet.state_dict(), MODEL_SAVE_PATH / 'facenet_resnet_vggface2.pt')\n",
    "print(f\"FaceNet model state_dict saved to {MODEL_SAVE_PATH / 'facenet_resnet_vggface2.pt'}\")\n",
    "\n",
    "# Example usage\n",
    "if image_files:\n",
    "    print(f\"\\nGenerating embeddings for sample image: {sample_image_path.name}\")\n",
    "    embeddings = generate_face_embeddings(sample_image_path)\n",
    "    if embeddings is not None:\n",
    "        print(f\"Generated {len(embeddings)} embeddings with shape {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db162557",
   "metadata": {},
   "source": [
    "## 5. Person Tracking (DeepSORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# DeepSORT is typically used in a real-time video processing pipeline.\n",
    "# It takes the bounding boxes from a detector (like YOLOv8) as input.\n",
    "\n",
    "# Initialize the tracker\n",
    "tracker = DeepSort(max_age=30) # max_age is the number of frames to keep a track alive without detection\n",
    "\n",
    "print(\"DeepSORT tracker initialized.\")\n",
    "\n",
    "# The integration of YOLOv8 + DeepSORT happens in the backend processing pipeline.\n",
    "# There isn't a specific 'model' file to save for DeepSORT itself, as it's an algorithm.\n",
    "# The backend will import and use the library directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125c4c0",
   "metadata": {},
   "source": [
    "## 6. Activity Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822abd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the most complex part and requires a custom-labeled dataset.\n",
    "# The process would be:\n",
    "# 1. For each tracked person, extract a sequence of frames (e.g., 16 frames).\n",
    "# 2. Extract pose information using a model like MMPose.\n",
    "# 3. Feed the sequence of poses or image crops into a classifier (e.g., a 3D-CNN, LSTM, or a Video Transformer like TimeSformer).\n",
    "# 4. Train the classifier on labeled data (e.g., 'typing', 'on_phone', 'talking').\n",
    "\n",
    "# Due to the complexity and lack of a pre-labeled public dataset for these specific activities,\n",
    "# we will outline the steps and prepare a placeholder for the model.\n",
    "\n",
    "print(\"Activity Classification: Model training pipeline needs to be built.\")\n",
    "print(\"This requires a custom dataset with labeled activities.\")\n",
    "\n",
    "# Placeholder for a future trained model\n",
    "placeholder_content = \"This is a placeholder for the activity classification model.\"\n",
    "with open(MODEL_SAVE_PATH / 'activity_classifier_placeholder.txt', 'w') as f:\n",
    "    f.write(placeholder_content)\n",
    "\n",
    "print(f\"Placeholder created at: {MODEL_SAVE_PATH / 'activity_classifier_placeholder.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35019a5d",
   "metadata": {},
   "source": [
    "## 7. Full Pipeline Simulation (Conceptual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d1dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def process_frame(frame):\n",
    "    # 1. Detect persons and desks using YOLOv8\n",
    "    detections = detection_model(frame)[0].boxes.data.cpu().numpy()\n",
    "    # Detections are in format [x1, y1, x2, y2, conf, class_id]\n",
    "    \n",
    "    person_detections = []\n",
    "    for det in detections:\n",
    "        if int(det[5]) == 0: # Class ID for 'person' in COCO is 0\n",
    "            bbox = det[:4]\n",
    "            conf = det[4]\n",
    "            person_detections.append((bbox, conf, 'person'))\n",
    "\n",
    "    # 2. Update tracker with person detections\n",
    "    tracks = tracker.update_tracks(person_detections, frame=frame)\n",
    "    \n",
    "    # 3. Process each track\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        \n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, (int(ltrb[0]), int(ltrb[1])), (int(ltrb[2]), int(ltrb[3])), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"ID: {track_id}\", (int(ltrb[0]), int(ltrb[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # 4. Face Recognition (on confirmed tracks, periodically)\n",
    "        # This would involve cropping the face, getting the embedding, and matching it to a database.\n",
    "        \n",
    "        # 5. Activity Classification (on confirmed tracks)\n",
    "        # This would involve feeding a sequence of frames/poses for this track_id to the classifier.\n",
    "        \n",
    "    return frame\n",
    "\n",
    "print(\"Conceptual pipeline function `process_frame` is defined.\")\n",
    "print(\"This logic will be implemented in the backend service.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
